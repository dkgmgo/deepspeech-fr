{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Since we're training our model on a google colab, we made this notebooks to start or continue the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init\n",
    "\n",
    "Init the objects used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.data import DataLoader, DataPrepocessor\n",
    "from util.model import Model\n",
    "from util.train import Trainer\n",
    "\n",
    "data_loader = DataLoader()\n",
    "preprocessor = DataPrepocessor(\n",
    "    frame_length=256, frame_step=160, fft_length=384, audio_path=data_loader.audio_path)\n",
    "\n",
    "train_dataset, validation_dataset = preprocessor.create_dataset_objets(\n",
    "    data_loader=data_loader, batch_size=30)  # choose carefully (too low means to much trainning time but to high means big computation)\n",
    "\n",
    "ds_model = Model(input_dim=preprocessor.fft_length//2 + 1,\n",
    "                    output_dim=preprocessor.char_to_num.vocabulary_size(), rnn_units=512)\n",
    "ds_model.model.summary(line_length=110)\n",
    "\n",
    "trainer = Trainer(model=ds_model.model, train_dataset=train_dataset,\n",
    "                      validation_dataset=validation_dataset, preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning\n",
    "\n",
    "Choose the number of epochs, default 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "epochs = 10\n",
    "save_every_n_hours = 3\n",
    "trainer.train(epochs=epochs, save_every_n_hours=save_every_n_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume training\n",
    "epochs = 10\n",
    "save_every_n_hours = 3\n",
    "trainer.model.load_weights('trainings/<latest_version>')\n",
    "trainer.train(epochs=epochs, save_every_n_hours=save_every_n_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Let's check the model on more validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from jiwer import wer\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "for batch in validation_dataset:\n",
    "    X, y = batch\n",
    "    batch_predictions = trainer.model.predict(X)\n",
    "    batch_predictions = trainer.decode_batch_predictions(batch_predictions)\n",
    "    predictions.extend(batch_predictions)\n",
    "    for label in y:\n",
    "        label = tf.strings.reduce_join(\n",
    "            trainer.preprocessor.num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        targets.append(label)\n",
    "wer_score = wer(targets, predictions)\n",
    "print(\"-\" * 100)\n",
    "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
    "print(\"-\" * 100)\n",
    "for i in np.random.randint(0, len(predictions), 10):\n",
    "    print(f\"Target    : {targets[i]}\")\n",
    "    print(f\"Prediction: {predictions[i]}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "To test the model with a speech wich is outside the dataset yo can try our web app at model_tester/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
